---
title: "Harvard Data Science - Capstone Project"
author: "Andrea De Nardi"
date: "2025-11-20"
output: pdf_document
---

```{r setup, include=FALSE, cache=TRUE}
# Setup and library installing / loading
knitr::opts_chunk$set(echo = TRUE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(knitr)

### Important note: This script assumes that the user created training and testing datasets following the edx snippet, and saved these datasets into a data folder, nested inside the current working directory. If this wasnt done, you can uncomment the following snippet and run it in order to create the datasets.
#
# library(languageserver)
#
# dl <- "ml-10M100K.zip"
# if(!file.exists(dl))
#   download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
# 
# ratings_file <- "ml-10M100K/ratings.dat"
# if(!file.exists(ratings_file))
#   unzip(dl, ratings_file)
# 
# movies_file <- "ml-10M100K/movies.dat"
# if(!file.exists(movies_file))
#   unzip(dl, movies_file)
# 
# ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
#                          stringsAsFactors = FALSE)
# colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
# ratings <- ratings %>%
#   mutate(userId = as.integer(userId),
#          movieId = as.integer(movieId),
#          rating = as.numeric(rating),
#          timestamp = as.integer(timestamp))
# 
# movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
#                         stringsAsFactors = FALSE)
# colnames(movies) <- c("movieId", "title", "genres")
# movies <- movies %>%
#   mutate(movieId = as.integer(movieId))
# 
# movielens <- left_join(ratings, movies, by = "movieId")
# 
# # Final hold-out test set will be 10% of MovieLens data
# set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# # set.seed(1) # if using R 3.5 or earlier
# test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
# edx <- movielens[-test_index,]
# temp <- movielens[test_index,]
# 
# # Make sure userId and movieId in final hold-out test set are also in edx set
# final_holdout_test <- temp %>% 
#   semi_join(edx, by = "movieId") %>%
#   semi_join(edx, by = "userId")
# 
# # Add rows removed from final hold-out test set back into edx set
# removed <- anti_join(temp, final_holdout_test)
# edx <- rbind(edx, removed)
# 
# rm(dl, ratings, movies, test_index, temp, movielens, removed)
# 
# # Save edx and holdout sets
# write.csv(edx, file = "data/edx.csv")
# write.csv(final_holdout_test, file = "data/final_holdout_test.csv")

# Load training data
edx <- read.csv("data/edx.csv")

# Load testing data
final_holdout_test = read.csv("data/final_holdout_test.csv")

set.seed(42)  # for reproducibility of our analysis
```

## Executive Summary

In this project, we will be working with the MovieLens dataset. This dataset contains millions of reviews given across thousands of movies. The goal of our analysis is to produce a movie recommendation system trained using the MovieLens data. To begin with, we will perform a brief exploratory analysis to understand what predictors can be used to adequately model movie ratings.

We will then tackle our modeling approach with a simple, naive model. We will build upon this baseline model to train more complex models in order to gradually reduce our prediction error. Our final model will be evaluated using a root mean square error approach, where we will compare the mean euclidian distance between our predictions and the true ratings.

## Analysis

Let us begin our analysis with some visualizations. Our first hypothesis is that users that give more reviews (movie buffs) may be more critical than those who have given less reviews. In order to check whether this intuition is true, we can take a sample of users, and create a scatterplot of their average rating vs the number of ratings they have given.

```{r, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, fig.height=3}
# 1. Pick, say, 5,000 random users
sample_users <- edx %>%
  distinct(userId) %>%
  slice_sample(n = 5000) %>%
  pull(userId)

# 2. Filter to just those users and compute per-user stats
user_stats_samp <- edx %>%
  filter(userId %in% sample_users) %>%
  group_by(userId) %>%
  summarise(
    number_of_reviews = n(),
    average_rating    = mean(rating),
    .groups = "drop"
  )

# 3. Scatterplot with LOESS (log x-axis)
ggplot(user_stats_samp, aes(x = number_of_reviews, y = average_rating)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "loess", se = TRUE) +
  scale_x_log10() +
  labs(
    x     = "Number of Reviews (log scale)",
    y     = "Average Rating",
    title = "Sampled Users: Review Activity vs. Mean Rating"
  ) +
  theme_minimal()

# There is perhaps a relationships for large number of reviews

```

The graphs highlights a slight negative relationship between the number of ratings given and the average rating. However, the practical significance seems quite limited. Let us now take a look at ratings across movie genres. Perhaps, certain types of movies tend to attract harsher reviewers that others?

```{r, echo=FALSE, cache=TRUE, fig.align='center'}
### Analyzing movie genres

# 1. Pull out the full list of genres
all_genres <- unique(unlist(strsplit(edx$genres, "\\|")))

# 2. For each genre, grepl() in the original string
for (g in all_genres) {
  col_name <- paste0("genre_", make.names(g))
  edx[[col_name]] <- grepl(g, edx$genres, fixed = TRUE)
}

orig_cols <- c("userId","movieId","rating","timestamp","title","genres")
genre_cols <- setdiff(names(edx), orig_cols)


# 3. Proportion of all ratings that are in each genre
prop_by_genre <- sapply(genre_cols, function(g) {
  mean(edx[[g]], na.rm = TRUE)
})

# 4. Average rating and count for each genre
mean_rating_by_genre <- sapply(genre_cols, function(g) {
  mean(edx$rating[edx[[g]]], na.rm = TRUE)
})
count_by_genre <- sapply(genre_cols, function(g) {
  sum(edx[[g]], na.rm = TRUE)
})

# 5. Combine into a summary data.frame
genre_summary <- data.frame(
  genre        = genre_cols,
  prop_ratings = prop_by_genre,
  mean_rating  = mean_rating_by_genre,
  count        = count_by_genre,
  row.names    = NULL) %>% arrange(-mean_rating)

# 6. View it
kable(genre_summary %>% filter(genre != "X"),
      digits = 4,
      caption = "Mean Rating by Movie Genre")

```

It seems that indeed, there are differences in average rating across movie genres. Likewise, it seems reasonable to posit that there are rating differences across users and across movies. Based on the above assumptions, we come up with the following plan to model our recommendation system:

1.  Split the data into a training and testing set.

2.  Train a naive model, where our predicted rating is simply the average rating across all movies, so the true rating is the average rating plus an error term: $Y_{u,i} = \mu + \varepsilon_{u,i}$

3.  Add an additional term to the naive model to take into account movie bias: $Y_{u,i} = \mu + b_i + \varepsilon_{u,i}$ , where the movie bias is defined as the difference between the average rating for a given movie and the average rating across all movies: $b_i = \frac{1}{n_i} \sum_{u \in i} (Y_{u,i} - \mu)$

4.  Add an additional term to factor in user bias: $Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}$, where the user bias is defined as $b_u = \frac{1}{n_u} \sum_{i \in u} (Y_{u,i} - \mu - b_i)$

5.  We will adjust this model by shrinking movie and user biases toward zero (“no effect”) when the number of observations is small. This is based on the assumption that movies or users with only a few ratings may skew the ratings due to low sample size (a user with only 2 movies rated 5 stars will have an artificially high $b_u$ and therefore inflate predicted ratings. In order to mitigate this, we can introduce a regularization term in the denominator of our movie bias and user bias, that will shrink biases for lower sample sizes. Our user bias and movie bias become: $b_u(\lambda) = \frac{\sum_{i} \left(Y_{u,i} - \mu - b_i(\lambda)\right)}{n_u + \lambda}$ and $b_i(\lambda) = \frac{\sum_{u} \left(Y_{u,i} - \mu\right)}{n_i + \lambda}$. The equation for movie rating is therefore: $\hat{Y}_{u,i} = \mu + b_i(\lambda) + b_u(\lambda)$.

6.  We will try a different approach and create a model using *recosystem*, an R library created specifically for the design of recommendation systems.

7.  Finally, we will evaluate our best performing model (the one with lowest training RMSE), by applying it to our testing data, and thus calculate the testing RMSE).

## Results

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
#
# MODELLING
#



#### Naive model Y = mu + error

# 1. Global mean on edx
mu <- mean(edx$rating)

# 2. Naive predictions = mu for every row
naive_pred <- rep(mu, nrow(edx))

# 3. RMSE on edx
train_naive_rmse <- sqrt(mean((edx$rating - naive_pred)^2))

# 4. Print results
# print(mu)               # the baseline constant prediction
# print(train_naive_rmse) # RMSE of that predictor on the training data



# RMSE of 1.06, not great but that's a good starting point


### Add movie bias: Y = mu + move_bias + error

# 1. Global mean
mu <- mean(edx$rating)

# 2. Movie biases (b_i)
bi <- edx %>%
  group_by(movieId) %>%
  summarise(b_i = mean(rating - mu), .groups = "drop")

# 3. Predictions with movie effect only
pred_movie <- edx %>%
  left_join(bi, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)

# 4. Compute training‐set RMSE
rmse_movie <- sqrt(mean((edx$rating - pred_movie)^2))

# That is better, with a training RMSE of 0.94

### Add user bias: Y = mu + movie_bias + user_bias + error

# 1. User biases (b_u) accounting for movie bias
bu <- edx %>%
  left_join(bi, by = "movieId") %>%
  group_by(userId) %>%
  summarise(b_u = mean(rating - mu - b_i), .groups = "drop")

# 2. Make predictions μ + b_i + b_u and compute RMSE
pred_um <- edx %>%
  left_join(bi, by = "movieId") %>%
  left_join(bu, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

rmse_um <- sqrt(mean((edx$rating - pred_um)^2))



### Regularize biases -> shrink them when little data is available for movie or user

# 1) Define a grid of lambda values to try
lambdas <- seq(0, 10, by = 0.5)

# 2) Function to compute training RMSE for a given lambda
rmse_train <- function(lambda) {
  mu <- mean(edx$rating)
  
  # movie biases with regularization
  bi <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu) / (n() + lambda), .groups="drop")
  
  # user biases with regularization
  bu <- edx %>%
    left_join(bi, by = "movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - mu - b_i) / (n() + lambda), .groups="drop")
  
  # assemble predictions on the full training set
  preds <- edx %>%
    left_join(bi, by = "movieId") %>%
    left_join(bu, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  # compute RMSE
  sqrt(mean((edx$rating - preds)^2))
}

# 3) Loop over lambdas and collect RMSEs
train_rmses <- sapply(lambdas, rmse_train)

# 4) Combine into a data.frame and print
train_summary <- data.frame(lambda = lambdas, train_rmse = train_rmses)

# 5) Retrieve lambda with lower train RMSE
lambda_fit <- train_summary$lambda[which.min(train_summary$train_rmse)]

# 6) Print training RMSE for this lambda
lambda_fit_rmse <- train_summary$train_rmse %>% min()



####
#### Further exploration : Try a recommentation system library
####

# The idea is to model the ratings as the inner product of two matrices:
# Q: The item factors
# P: The user factors
# Under the hood, recosystems uses an alternating minimization / coordinate descent approach
# It finds the P and Q that minimize the regularized squared-error over all known ratings.


if (!requireNamespace("recosystem", quietly = TRUE)) {
  install.packages("recosystem")
}
library(recosystem)

# edx %>% str()

# 1. Prepare the in‐memory train set for recosystem (according to package documentation)
train_data <- data_memory(
  user_index = edx$userId,
  item_index = edx$movieId,
  rating     = edx$rating
)

# 2. Create and train the Reco model (we will pick a set or default hyperparameters, no hyperparameter tuning for the moment)
r <- Reco()
r$train(
  train_data,
  opts = list(
    dim       = 10,
    lrate     = 0.1,
    costp_l2  = 0.01,
    costq_l2  = 0.01,
    niter     = 10,
    verbose   = FALSE
  )
)

# 3. Predict back on the full training set
train_preds <- r$predict(train_data, out_memory())

# 4. Compute training RMSE
train_rmse <- RMSE(train_preds, edx$rating)


# Presenting the training RMSE's in a table:
rmse_results <- tibble(
  Model = c(
    "Naive mean model",
    "Movie effect model",
    "Movie + user effect model",
    "Regularized movie + user model",
    "Matrix factorization (recosystem)"
  ),
  Training_RMSE = c(
    train_naive_rmse,
    rmse_movie,
    rmse_um,
    lambda_fit_rmse,
    train_rmse
  )
)

kable(
  rmse_results,
  digits = 4,
  caption = "Training RMSE Comparison Across Models"
)

```

As we can see, our naive model provides a solid baseline with a training RMSE of: `r round(train_naive_rmse,4)`. Taking into account movie-specific and user-specific effects helps increase the performance, with an RMSE reduced to `r round(rmse_um,4)`. We further improve the model by shrinking down movie and user biases for low sample sizes (regularization), but the gain is minimal. The most significant improvement is obtained with the *recosystem* library, which models the ratings as the inner product of two matrices (item matrix and user matrix) obtained by minimizing error over the known ratings in the training dataset. The resulting model has a training RMSE of `r round(train_rmse,4)`.

```{r, cache=TRUE, echo=FALSE}
####
#### Testing error
####


# 1. Prepare the test data for recosystem (no ratings needed for predict)
test_data <- data_memory(
  user_index = final_holdout_test$userId,
  item_index = final_holdout_test$movieId
)

# 2. Generate predictions on the test set
test_preds <- r$predict(test_data, out_memory())

# 3. Compute and print the RMSE on final_holdout_test
test_rmse <- RMSE(test_preds, final_holdout_test$rating)
# print(test_rmse)

# Print RMSE
kable(
  data.frame(Test_RMSE = round(test_rmse, 4)),
  caption = "Final Hold-Out Test RMSE - Recosystem Model"
)


```

After applying the model to the test data, we can evaluate the performance of our recommendation system on fresh new data, and we find a final test RMSE of `r round(test_rmse,4)`.

## Conclusion

In this project, we developed a recommendation system using the MovieLens dataset by progressively increasing the complexity of our models and evaluating their performance. Beginning with a naive baseline, we incorporated movie-specific and user-specific effects, and subsequently applied regularization to mitigate the impact of low-sample biases. While these incremental improvements yielded modest gains, the most substantial reduction in prediction error was achieved using a matrix factorization approach via the *recosystem* library. This method, which models user–item interactions through latent factors, produced the lowest error on both the training data and the final hold-out test set, with a test RMSE of **`r sprintf('%.4f', test_rmse)`**. Overall, our results highlight the value of latent-factor models in recommendation systems and demonstrate that even simple bias-based models can capture meaningful structure in user rating behavior. Future improvements could include hyperparameter tuning, cross-validation, and incorporating temporal effects to further refine predictive accuracy.
